# DAPG: Distance-Aware Pruned Graph

## Anonymous review artifact. This repository contains the reference implementation of DAPG, a single-layer, geometry-aware ANN index introduced in the paper.


## **Abstract**

DAPG introduces percentile-based local filtering and adaptive global sparsification to build degree-adaptive proximity graphs that preserve reachability while reducing redundant edges.  
DAPG improves latencyâ€“recall trade-offs over **state-of-the-art (SOTA)** baselines without multi-layer indexing.

<p>
  <kbd>+3.3% recall</kbd>
  <kbd>2.9Ã— faster</kbd>
  <kbd>Single layer</kbd>
  <kbd>LSH seeding</kbd>
</p>

---

## Introduction

This repository provides the source code for **DAPG**, a novel graph-based Approximate Nearest Neighbor (ANN) indexing system introduced in the paper.

---


## What DAPG Adds - Method

**Distance-Aware Local Pruning (percentile threshold per node).**

**Adaptive Global Sparsification (cap high-degree tails.**

**Parallel construction with thread-safe updates.**

**Serialization of graph + LSH structures for fast reloads.**

---
## **Cost Model**

**Local percentile threshold  + adaptive cap <code>T'</code>.**
**LSH seeding; no hierarchy.**

**Supports dynamic insertion and deletion with degree-stable connectivity.**

Expected query cost factorizes into average degree and expansion depth.

<pre>
C_Q = O(&macr;d_DAPG Â· Î²(â„“))
T_Q = O(d Â· &macr;d_DAPG Â· Î²(â„“))
Î²(â„“) = O(log n) under small-world routing
</pre>

**Cost model: <code>C<sub>Q</sub> = O(&macr;d<sub>DAPG</sub>&nbsp;Î²(â„“))</code>.**

---

## Contributions

## What We Bring
<table> <tr> <td width="48%" valign="top">

1) Theory
Formalizes distance-aware pruning and adaptive degree control in proximity graphs, providing probabilistic bounds on reachability and connectivity under percentile-based sparsification.

</td> <td width="48%" valign="top">

2) Method
DAPG introduces local percentile filtering (P<sub>local</sub>) and global capping (P<sub>global</sub>) to construct degree-adaptive graphs that minimize redundant edges while maintaining recall.

</td> </tr> <tr> <td width="48%" valign="top">

3) Empirics
Outperforms LSH-APG across DEEP1M, MNIST, and SIFT1M, achieving up to +3.3% recall and 2.9Ã— lower query latency, while reducing graph density and index memory.

</td> <td width="48%" valign="top">

4) Guidance
Provides design rules for local/global pruning rates, ensuring consistent recall stability under dynamic insertions and scalable O(d C<sub>Q</sub>) maintenance.

</td> </tr> </table>


## Compilation

The code is implemented in **C++11** and supports parallelism using **OpenMP**. It can be compiled on both Linux and Windows.

### Linux

Linux (g++ / clang++)

Requires C++11 and OpenMP.

The provided Makefile auto-detects -fopenmp. If your toolchain differs, edit cppCode/DAPG/Makefile.

```bash
cd ./cppCode/DAPG
make
```

###  Windows

Use **Visual Studio 2019+** to import the project located in:

```
./cppCode/DAPG/src/
```

Make sure to enable OpenMP and C++11 support in the build settings.

## Running DAPG

### Command Format

```bash
./dapg datasetName
```

- `datasetName`: The name of the dataset (e.g., `sift`, `mnist`, `audio`)

### Example

```bash
cd ./cppCode/DAPG
./dapg sift
```

This runs DAPG index construction and search on the `sift` dataset.

## Key Features

-  Distance-Aware Local Pruning: Adapts edge filtering based on local percentile distances.
-  Sparsity Control: Limits node degree while preserving connectivity in sparse regions.
-  Improved Recallâ€“Latency Tradeoff: Reduces query time without degrading recall.
-  Compatible with ANN frameworks.

## Dataset Format

The expected input format is a binary file containing float vectors, structured as:

```
{int: float size in bytes}
{int: number of vectors}
{int: dimension}
{float[]: all vector values, stored sequentially}
```

### Example: `sift.data_new`

To use your dataset:

1. Convert it into the binary format shown above.
2. Rename it as `[datasetName].data_new`
3. Place it in: `./dataset/`

A sample dataset (e.g., `audio.data_new`) is already provided.

## Datasets

We support and have tested DAPG on:

- [Audio](https://github.com/RSIA-LIESMARS-WHU/LSHBOX-sample-data)
- [SIFT1M](http://corpus-texmex.irisa.fr/)
- [Deep1M](https://www.cse.cuhk.edu.hk/systems/hash/gqr/dataset/deep1M.tar.gz)
- [MNIST](http://yann.lecun.com/exdb/mnist/)
- [SIFT100M](http://corpus-texmex.irisa.fr/)


Convert these into the `.data_new` format for compatibility.

## System Setup

Our experiments were conducted on both local and cloud-based environments to evaluate the efficiency and scalability of the DAPG system.

###  Local Workstation
- **Processor**: 13th Gen IntelÂ® Coreâ„¢ i9-13900HX (24 cores, 32 threads)
- **Base Frequency**: 2.2 GHz  
- **OS**: Ubuntu 20.04 LTS  
- **Precision**: `float32` for all vectors  
- **Implementation**: C++ with multi-threading via OpenMP  
- **Query Setup**: 104 queries per experiment, averaged over 5 independent runs

###  Microsoft Azure Virtual Machines

1. **Standard F32s v2**
   - **vCPUs**: 32  
   - **RAM**: 64 GiB  
   - **Dataset**: SIFT1M  
   - **Purpose**: Scalability evaluation

2. **Standard E64-32s v3 (High-Memory)**
   - **vCPUs**: 32 (IntelÂ® XeonÂ® Platinum 8272CL)  
   - **RAM**: 432 GiB  
   - **Disk**: 1 TB Premium SSD  
   - **OS**: Ubuntu 22.04 LTS  
   - **Dataset**: SIFT100M  
   - **Purpose**: Large-scale indexing and ANN benchmarking

This high-memory configuration allowed for efficient scaling to large datasets, and multi-threaded execution ensured fast parallel processing during both index construction and query search.
 

#### Distance-Aware Pruning (DAP)
- Introduced a percentile-based thresholding mechanism.
- For each node, computed the 80th percentile distance (Ï„_q) over LSH candidates.
- Inserted only neighbors with `dist < Ï„_q` to ensure sparsity and relevance.
- Exposed `last_threshold` for optional diagnostics or debugging.


####  Parallel Construction
- Used `ParallelFor` to insert all nodes in parallel (except the first).
- Enabled thread safety via `std::shared_mutex` for concurrent graph updates.
- Fallbacks to `std::mutex` when C++17 is not available.

#### Serialization
- Graph (`linkLists`) and LSH hash tables are saved to and loaded from a binary format.
- Implemented in `save()` and the constructor `divGraph(Preprocess* prep, ...)`.

## Benchmark Logs

Each row logs detailed metrics:

- **Recall**: Top-k retrieval accuracy
- **Pruning**: Ratio of retained neighbors after DAP-based filtering
- **Time**, **Cost**, and additional performance indicators
- **Algorithm Name**: Includes pruning threshold information (e.g., `DAP_k10_th...`)


### Experimental

- The header includes configuration details such as:  
  `k=20, probQ=0.9, L=2, K=18, T=24`

Each row records:
- `algName`: algorithm configuration with pruning threshold
- `k`: number of neighbors
- `ef`: search parameter
- `Time`: average query time (ms)
- `Recall`: search recall
- `Cost`, `CPQ1`, `CPQ2`: computation cost metrics
- `Pruning`: pruning ratio applied

### Parameter Settings

We evaluate DAP (Distance-Aware Pruning) with:

- k=20, L=2, K=18, T=24, Tâ€²=48, W=1.0, pC=0.95, pQ=0.90, efC=80

  
**DAPG computes Ï„_q per node (default percentile = 80).**


DAP applies **local dynamic pruning**, computing a threshold `Ï„_q` per node.

---

### Evaluation Setup

We evaluate DAPG across a range of:

- `k âˆˆ {1, 10, 20, ..., 100}`
- `ef` values for query expansion

This allows robust analysis of recall and efficiency across diverse search settings

## Logs & Metrics
Each run logs:

Recall@k, Time(ms), Cost, CPQ*, Pruning(%)

algName encodes the pruning threshold (e.g., DAP_k10_th80)

Seed and environment are printed at the top for determinism.

## Empirical evidence


## Complexity and Efficiency Comparison

**DAPG** achieves lower query complexity and higher efficiency than LSH-APG and HNSW-style baselines.  
Its percentile-based local filtering and adaptive global sparsification yield a *degree-adaptive graph*
with reduced average degree while preserving reachability.

- **Build:** ð’ªÌƒ(n dÌ„_seed) â†’ dÌ„_DAPG ð’ª(dÌ„_DAPG Î²(â„“))  
- **Query:** ð’ª(d C_Q) amortized per query  
- **No multi-layer structure** â†’ smaller memory footprint and lower update cost  
- **Empirical:** up to **2.9Ã— faster** and **+3.3 % higher recall** than LSH-APG



**Complexity Comparison**

| Method | Build Complexity | Query Complexity | Notes |
|:-------|:----------------:|:----------------:|:------|
| HNSW | Ã•(n d) | O(L dÌ„ Î²(â„“)) | Multi-layer; log n levels |
| LSH-APG | Ã•(n d) | O(dÌ„ Î²(â„“)) | Fixed-degree pruning |
| **DAPG (Ours)** | Ã•(n d<sub>seed</sub>) â†’ dÌ„<sub>DAPG</sub> | **O(dÌ„<sub>DAPG</sub> Î²(â„“))** | Local percentile + cap Tâ€² |


- **Fewer edges per node** â†’ less traversal per query  
- **Adaptive pruning** â†’ lower redundancy  
- **Single-layer design** â†’ reduced memory vs. multi-layer HNSW  
- **Dynamic updates** â†’ amortized `O(d C_Q)` maintenance  


| **Dataset** | **Recall@10** | **Query Time (ms)** | **Improvement vs LSH-APG** |
|--------------|---------------|---------------------|-----------------------------|
| **DEEP1M** | 0.9632 vs 0.9590 | 2.30 vs 3.43 | +0.44 % recall  /  32.9 % faster |
| **MNIST** | 0.9984 vs 0.9972 | 0.56 vs 0.68 | +0.12 % recall  /  17.9 % faster |
| **SIFT1M** | 0.9870 vs 0.9580 | 0.83 vs 2.42 | +3.34 % recall  /  2.9Ã— faster |

> DAPG achieves higher recall **and** lower latency on every dataset.


Performance comparison of DAPG vs. LSH-APG on DEEP1M, MNIST, and SIFT1M.
DAPG continues to achieve higher recall and lower query latency than LSH-APG across all datasets, demonstrating consistent scalability and efficiency gains at larger neighborhood sizes.
<table>
<tr>
<td>

<b>Results at k = 10</b>

| Dataset | Method | Recall@10 | Query Time (ms) | Index Size (MB) |
|----------|---------|------------|----------------|----------------|
| **DEEP1M** | LSH-APG | 0.9590 | 3.43 | 250 |
|  | **DAPG (Ours)** | **0.9632 Â± 0.0057** | **2.30 Â± 0.20** | **449** |
| **MNIST** | LSH-APG | 0.9972 | 0.682 | 10 |
|  | **DAPG (Ours)** | **0.9984 Â± 0.0010** | **0.560 Â± 0.290** | **27.77** |
| **SIFT1M** | LSH-APG | 0.9580 | 2.42 | 468 |
|  | **DAPG (Ours)** | **0.9870 Â± 0.0025** | **0.83 Â± 0.00** | **455** |

</td>
<td>

<b>Results at k = 50</b>

| Dataset | Method | Recall@50 | Query Time (ms) | Index Size (MB) |
|----------|---------|------------|----------------|----------------|
| **DEEP1M** | LSH-APG | 0.9590 | 3.43 | 250 |
|  | **DAPG (Ours)** | **0.9615 Â± 0.0042** | **2.30 Â± 0.22** | **449** |
| **MNIST** | LSH-APG | 0.9972 | 0.682 | 10 |
|  | **DAPG (Ours)** | **0.9984 Â± 0.0010** | **0.560 Â± 0.290** | **27.77** |
| **SIFT1M** | LSH-APG | 0.9580 | 2.42 | 468 |
|  | **DAPG (Ours)** | **0.9738 Â± 0.0028** | **0.83 Â± 0.00** | **455** |

</td>
</tr>
</table>

## Comparison

| **Aspect** | **LSH-APG** | **DAPG (Ours)** | **Outcome** |
|:--|:--|:--|:--|
| **Query Complexity** | `O(d Î²(â„“))` | **O(dÌ„<sub>DAPG</sub> Î²(â„“)) â‰ˆ O(d C<sub>Q</sub>)** | Same asymptotic form but smaller constant |
| **Degree Control** | Fixed (no adaptivity) | **Adaptive via percentile P<sub>local</sub> + global cap P<sub>global</sub>** | Controllable sparsity |
| **Update Cost** | Rebuild required | **Incremental O(d C<sub>Q</sub>)** (amortized) | dynamic updates |
| **Proof Coverage** | Heuristic | **Formal bounds on reachability & connectivity (Lemma 3, Thm 2)** | Theoretical guarantees |

Compared to existing ANN frameworks such as HNSW, NSG, DB-LSH, and LSH-APG, DAPG provides the only single-layer hybrid structure with both a formally proven expected query complexity and a probabilistic connectivity guarantee, ensuring sublinear query cost and robust reachability under adaptive sparsification.
> **As shown in Figure 8 (paper)**, DAPG reduces index memory by up to **10Ã—** compared to HNSW and NSG on SIFT100M, while maintaining comparable build time.  
>  
> **DAPG demonstrates higher efficiency** than prior ANN frameworks, reducing query latency by up to **2.9Ã—**, lowering memory footprint by up to **10Ã—**, and maintaining comparable build cost, while preserving the same theoretical complexity  
> **O(dÌ„<sub>DAPG</sub> Î²(â„“))** and achieving consistently higher recall.

## Research Project Directory Structure

```
.
â”œâ”€â”€ .vscode/                         # VS Code settings
â”œâ”€â”€ Report/                          # Project report and documentation
â”œâ”€â”€ cppCode/                         # Main C++ codebase
â”‚   â”œâ”€â”€ src/                        # Modified: Distance-Aware Pruned Graph implementation
â”‚                 
    
â”‚       â”œâ”€â”€ indexes/ mnist_all_index_stats.txt  # Precomputed or saved index files
â”‚       â”œâ”€â”€ /divGraph.h             # Modified: Updated source files 
â”‚       â”œâ”€â”€ Makefile                  # Build configuration
â”‚       â”œâ”€â”€ main.cpp       
â”‚       â”œâ”€â”€ Query.cpp
        â”œâ”€â”€ dapgalg.h

â”‚                         
â”‚       
â”œâ”€â”€ dataset/                         
â”œâ”€â”€ .gitattributes                   
â”œâ”€â”€ .gitignore                       
â”œâ”€â”€ LICENSE                          
â””â”€â”€ README.md                       
```
## Contact

For questions or contributions, please open an issue or contact the authors listed in the paper.


