# DAPG: Distance-Aware Pruned Graph

## Anonymous review artifact. This repository contains the reference implementation of DAPG, a single-layer, geometry-aware ANN index introduced in the paper.


## **ABSTRACT**

DAPG introduces percentile-based local filtering and adaptive global sparsification to build degree-adaptive proximity graphs that preserve reachability while reducing redundant edges.  
DAPG improves latency‚Äìrecall trade-offs over **state-of-the-art (SOTA)** baselines without multi-layer indexing.

<p>
  <kbd>+3.3% recall</kbd>
  <kbd>2.9√ó faster</kbd>
  <kbd>Single layer</kbd>
  <kbd>LSH seeding</kbd>
</p>

---
## Why This Work

### What existing ANN methods miss ‚Äî and how **DAPG** fixes it

|  Common gaps in existing ANN methods |  How **DAPG** addresses these |
| :----------------------------------- | :------------------------------ |
| ‚Ä¢ **Fixed-degree graphs:** Static degree limits prevent adaptive sparsification. |  **Adaptive sparsity:** percentile-based local filtering and global capping balance recall vs. cost. |
| ‚Ä¢ **Costly rebuilds:** Traditional structures require full index reconstruction for new data. |  **Incremental updates:** supports amortized insertion with bounded rewire cost. |
| ‚Ä¢ **Uncontrolled expansion:** Greedy traversal often expands redundant nodes. |  **Pruned search:** expansion is capped by Œ≤(‚Ñì), maintaining efficiency. |
| ‚Ä¢ **Weak theoretical link:** Prior heuristics lack formal sublinear complexity bounds. |  **Theory-backed:** Lemma 3 + Lemma 2 prove sublinear query complexity O(dÃÑ<sub>DAPG</sub> Œ≤(‚Ñì)) and bounded connectivity. |

> **Result:** Higher recall (+3.3%), 2.9√ó lower query latency, and up to 10√ó smaller memory footprint‚Äîwhile preserving the same O(dÃÑ<sub>DAPG</sub> Œ≤(‚Ñì)) complexity.

---
## Contributions

## What We Bring
<table> <tr> <td width="48%" valign="top">

1) Theory
Formalizes distance-aware pruning and adaptive degree control in proximity graphs, providing probabilistic bounds on reachability and connectivity under percentile-based sparsification.

</td> <td width="48%" valign="top">

2) Method
DAPG introduces local percentile filtering (P<sub>local</sub>) and global capping (P<sub>global</sub>) to construct degree-adaptive graphs that minimize redundant edges while maintaining recall.

</td> </tr> <tr> <td width="48%" valign="top">

3) Empirics
Outperforms LSH-APG across DEEP1M, MNIST, and SIFT1M, achieving up to +3.3% recall and 2.9√ó lower query latency, while reducing graph density and index memory.

</td> <td width="48%" valign="top">

4) Guidance
Provides design rules for local/global pruning rates, ensuring consistent recall stability under dynamic insertions and scalable O(d C<sub>Q</sub>) maintenance.

</td> </tr> </table>


## INTRODUCTION
This repository provides the source code for **DAPG**, a novel graph-based Approximate Nearest Neighbor (ANN) indexing system introduced in the paper.

---


## What DAPG Adds - DISTANCE-AWARE PRUNED GRAPH FRAMEWORK

**Distance-Aware Local Pruning (percentile threshold per node).**

**Adaptive Global Sparsification (cap high-degree tails.**

**Parallel construction with thread-safe updates.**

**Serialization of graph + LSH structures for fast reloads.**

---
## **Cost Model**

**Local percentile threshold  + adaptive cap <code>T'</code>.**
**LSH seeding; no hierarchy.**

**Supports dynamic insertion and deletion with degree-stable connectivity.**

Expected query cost factorizes into average degree and expansion depth.

<pre>
C_Q = O(&macr;d_DAPG ¬∑ Œ≤(‚Ñì))
T_Q = O(d ¬∑ &macr;d_DAPG ¬∑ Œ≤(‚Ñì))
Œ≤(‚Ñì) = O(log n) under small-world routing
</pre>

**Cost model: <code>C<sub>Q</sub> = O(&macr;d<sub>DAPG</sub>&nbsp;Œ≤(‚Ñì))</code>.**

---



## Compilation

The code is implemented in **C++11** and supports parallelism using **OpenMP**. It can be compiled on both Linux and Windows.

### Linux

Linux (g++ / clang++)

Requires C++11 and OpenMP.

The provided Makefile auto-detects -fopenmp. If your toolchain differs, edit cppCode/DAPG/Makefile.

```bash
cd ./cppCode/DAPG
make
```

###  Windows

Use **Visual Studio 2019+** to import the project located in:

```
./cppCode/DAPG/src/
```

Make sure to enable OpenMP and C++11 support in the build settings.

## Running DAPG

### Command Format

```bash
./dapg datasetName
```

- `datasetName`: The name of the dataset (e.g., `sift`, `mnist`, `audio`)

### Example

```bash
cd ./cppCode/DAPG
./dapg sift
```

This runs DAPG index construction and search on the `sift` dataset.

## Key Features

-  Distance-Aware Local Pruning: Adapts edge filtering based on local percentile distances.
-  Sparsity Control: Limits node degree while preserving connectivity in sparse regions.
-  Improved Recall‚ÄìLatency Tradeoff: Reduces query time without degrading recall.
-  Compatible with ANN frameworks.
  
## Datasets

We support and have tested DAPG on:

- [Audio](https://github.com/RSIA-LIESMARS-WHU/LSHBOX-sample-data)
- [SIFT1M](http://corpus-texmex.irisa.fr/)
- [Deep1M](https://www.cse.cuhk.edu.hk/systems/hash/gqr/dataset/deep1M.tar.gz)
- [MNIST](http://yann.lecun.com/exdb/mnist/)
- [SIFT100M](http://corpus-texmex.irisa.fr/)


Convert these into the `.data_new` format for compatibility.
## Dataset Format

The expected input format is a binary file containing float vectors, structured as:

```
{int: float size in bytes}
{int: number of vectors}
{int: dimension}
{float[]: all vector values, stored sequentially}
```

### Example: `sift.data_new`

To use your dataset:

1. Convert it into the binary format shown above.
2. Rename it as `[datasetName].data_new`
3. Place it in: `./dataset/`

A sample dataset (e.g., `audio.data_new`) is already provided.


## EVALUATIONS
## System Setup

Our experiments were conducted on both local and cloud-based environments to evaluate the efficiency and scalability of the DAPG system.

###  Local Workstation
- **Processor**: 13th Gen Intel¬Æ Core‚Ñ¢ i9-13900HX (24 cores, 32 threads)
- **Base Frequency**: 2.2 GHz  
- **OS**: Ubuntu 20.04 LTS  
- **Precision**: `float32` for all vectors  
- **Implementation**: C++ with multi-threading via OpenMP  
- **Query Setup**: 104 queries per experiment, averaged over 5 independent runs

###  Microsoft Azure Virtual Machines

1. **Standard F32s v2**
   - **vCPUs**: 32  
   - **RAM**: 64 GiB  
   - **Dataset**: SIFT1M  
   - **Purpose**: Scalability evaluation

2. **Standard E64-32s v3 (High-Memory)**
   - **vCPUs**: 32 (Intel¬Æ Xeon¬Æ Platinum 8272CL)  
   - **RAM**: 432 GiB  
   - **Disk**: 1 TB Premium SSD  
   - **OS**: Ubuntu 22.04 LTS  
   - **Dataset**: SIFT100M  
   - **Purpose**: Large-scale indexing and ANN benchmarking

This high-memory configuration allowed for efficient scaling to large datasets, and multi-threaded execution ensured fast parallel processing during both index construction and query search.
 

#### Distance-Aware Pruning (DAP)
- Introduced a percentile-based thresholding mechanism.
- For each node, computed the 80th percentile distance (œÑ_q) over LSH candidates.
- Inserted only neighbors with `dist < œÑ_q` to ensure sparsity and relevance.
- Exposed `last_threshold` for optional diagnostics or debugging.


####  Parallel Construction
- Used `ParallelFor` to insert all nodes in parallel (except the first).
- Enabled thread safety via `std::shared_mutex` for concurrent graph updates.
- Fallbacks to `std::mutex` when C++17 is not available.

#### Serialization
- Graph (`linkLists`) and LSH hash tables are saved to and loaded from a binary format.
- Implemented in `save()` and the constructor `divGraph(Preprocess* prep, ...)`.

## Benchmark Logs

Each row logs detailed metrics:

- **Recall**: Top-k retrieval accuracy
- **Pruning**: Ratio of retained neighbors after DAP-based filtering
- **Time**, **Cost**, and additional performance indicators
- **Algorithm Name**: Includes pruning threshold information (e.g., `DAP_k10_th...`)


- The header includes configuration details such as:  
  `k=20, probQ=0.9, L=2, K=18, T=24`

Each row records:
- `algName`: algorithm configuration with pruning threshold
- `k`: number of neighbors
- `ef`: search parameter
- `Time`: average query time (ms)
- `Recall`: search recall
- `Cost`, `CPQ1`, `CPQ2`: computation cost metrics
- `Pruning`: pruning ratio applied

### Parameter Settings

We evaluate DAP (Distance-Aware Pruning) with:

- k=20, L=2, K=18, T=24, T‚Ä≤=48, W=1.0, pC=0.95, pQ=0.90, efC=80

  
**DAPG computes œÑ_q per node (default percentile = 80).**


DAP applies **local dynamic pruning**, computing a threshold `œÑ_q` per node.

---

We evaluate DAPG across a range of:

- `k ‚àà {1, 10, 20, ..., 100}`
- `ef` values for query expansion

This allows robust analysis of recall and efficiency across diverse search settings

## Metrics
Each run logs:

Recall@k, Time(ms), Cost, CPQ*, Pruning(%)

algName encodes the pruning threshold (e.g., DAP_k10_th80)

Seed and environment are printed at the top for determinism.


## Complexity and Efficiency Comparison

> **Lemma 3** (supported by **Lemma 2**) proves that DAPG achieves sublinear query complexity  
> **O(dÃÑ<sub>DAPG</sub> Œ≤(‚Ñì))** with bounded degree and probabilistic connectivity,  
> ensuring both **efficiency** and **graph connectivity**.


**DAPG** achieves lower query complexity and higher efficiency than LSH-APG and HNSW-style baselines.  
Its percentile-based local filtering and adaptive global sparsification yield a *degree-adaptive graph*
with reduced average degree while preserving reachability.

- **Build:** ùí™ÃÉ(n dÃÑ_seed) ‚Üí dÃÑ_DAPG ùí™(dÃÑ_DAPG Œ≤(‚Ñì))  
- **Query:** ùí™(d C_Q) amortized per query  
- **No multi-layer structure** ‚Üí smaller memory footprint and lower update cost  
- **Empirical:** up to **2.9√ó faster** and **+3.3 % higher recall** than LSH-APG


| Method | Build Complexity | Query Complexity | Notes |
|:-------|:----------------:|:----------------:|:------|
| HNSW | √ï(n d) | O(L dÃÑ Œ≤(‚Ñì)) | Multi-layer; log n levels |
| LSH-APG | √ï(n d) | O(dÃÑ Œ≤(‚Ñì)) | Fixed-degree pruning |
| **DAPG (Ours)** | √ï(n d<sub>seed</sub>) ‚Üí dÃÑ<sub>DAPG</sub> | **O(dÃÑ<sub>DAPG</sub> Œ≤(‚Ñì))** | Local percentile + cap T‚Ä≤ |


- **Fewer edges per node** ‚Üí less traversal per query  
- **Adaptive pruning** ‚Üí lower redundancy  
- **Single-layer design** ‚Üí reduced memory vs. multi-layer HNSW  
- **Dynamic updates** ‚Üí amortized `O(d C_Q)` maintenance  


| **Dataset** | **Recall@10** | **Query Time (ms)** | **Improvement vs LSH-APG** |
|--------------|---------------|---------------------|-----------------------------|
| **DEEP1M** | 0.9632 vs 0.9590 | 2.30 vs 3.43 | +0.44 % recall  /  32.9 % faster |
| **MNIST** | 0.9984 vs 0.9972 | 0.56 vs 0.68 | +0.12 % recall  /  17.9 % faster |
| **SIFT1M** | 0.9870 vs 0.9580 | 0.83 vs 2.42 | +3.34 % recall  /  2.9√ó faster |

> DAPG achieves higher recall **and** lower latency on every dataset.


Performance comparison of DAPG vs. LSH-APG on DEEP1M, MNIST, and SIFT1M.
DAPG continues to achieve higher recall and lower query latency than LSH-APG across all datasets, demonstrating consistent scalability and efficiency gains at larger neighborhood sizes.
<table>
<tr>
<td>

<b>Results at k = 10</b>

| Dataset | Method | Recall@10 | Query Time (ms) | Index Size (MB) |
|----------|---------|------------|----------------|----------------|
| **DEEP1M** | LSH-APG | 0.9590 | 3.43 | 250 |
|  | **DAPG (Ours)** | **0.9632 ¬± 0.0057** | **2.30 ¬± 0.20** | **449** |
| **MNIST** | LSH-APG | 0.9972 | 0.682 | 10 |
|  | **DAPG (Ours)** | **0.9984 ¬± 0.0010** | **0.560 ¬± 0.290** | **27.77** |
| **SIFT1M** | LSH-APG | 0.9580 | 2.42 | 468 |
|  | **DAPG (Ours)** | **0.9870 ¬± 0.0025** | **0.83 ¬± 0.00** | **455** |

</td>
<td>

<b>Results at k = 50</b>

| Dataset | Method | Recall@50 | Query Time (ms) | Index Size (MB) |
|----------|---------|------------|----------------|----------------|
| **DEEP1M** | LSH-APG | 0.9590 | 3.43 | 250 |
|  | **DAPG (Ours)** | **0.9615 ¬± 0.0042** | **2.30 ¬± 0.22** | **449** |
| **MNIST** | LSH-APG | 0.9972 | 0.682 | 10 |
|  | **DAPG (Ours)** | **0.9984 ¬± 0.0010** | **0.560 ¬± 0.290** | **27.77** |
| **SIFT1M** | LSH-APG | 0.9580 | 2.42 | 468 |
|  | **DAPG (Ours)** | **0.9738 ¬± 0.0028** | **0.83 ¬± 0.00** | **455** |

</td>
</tr>
</table>

## Comparison

| **Aspect** | **LSH-APG** | **DAPG (Ours)** | **Outcome** |
|:--|:--|:--|:--|
| **Query Complexity** | `O(d Œ≤(‚Ñì))` | **O(dÃÑ<sub>DAPG</sub> Œ≤(‚Ñì)) ‚âà O(d C<sub>Q</sub>)** | Same asymptotic form but smaller constant |
| **Degree Control** | Fixed (no adaptivity) | **Adaptive via percentile P<sub>local</sub> + global cap P<sub>global</sub>** | Controllable sparsity |
| **Update Cost** | Rebuild required | **Incremental O(d C<sub>Q</sub>)** (amortized) | dynamic updates |
| **Proof Coverage** | Heuristic | **Formal bounds on reachability & connectivity** | Theoretical Analysis |

Compared to existing ANN frameworks such as HNSW, NSG, DB-LSH, and LSH-APG, DAPG provides the only single-layer hybrid structure with both a formally proven expected query complexity and a probabilistic connectivity guarantee, ensuring sublinear query cost and robust reachability under adaptive sparsification.
> **As shown in Figure 8 (paper)**, DAPG reduces index memory by up to **10√ó** compared to HNSW and NSG on SIFT100M, while maintaining comparable build time.  
>  
> **DAPG demonstrates higher efficiency** than prior ANN frameworks, reducing query latency by up to **2.9√ó**, lowering memory footprint by up to **10√ó**, and maintaining comparable build cost, while preserving the same theoretical complexity  
> **O(dÃÑ<sub>DAPG</sub> Œ≤(‚Ñì))** and achieving consistently higher recall.

## Research Project Directory Structure

```
.
‚îú‚îÄ‚îÄ .vscode/                         # VS Code settings
‚îú‚îÄ‚îÄ Report/                          # Project report and documentation
‚îú‚îÄ‚îÄ cppCode/                         # Main C++ codebase
‚îÇ   ‚îú‚îÄ‚îÄ src/                        # Modified: Distance-Aware Pruned Graph implementation
‚îÇ                 
    
‚îÇ       ‚îú‚îÄ‚îÄ indexes/ mnist_all_index_stats.txt  # Precomputed or saved index files
‚îÇ       ‚îú‚îÄ‚îÄ /divGraph.h             # Modified: Updated source files 
‚îÇ       ‚îú‚îÄ‚îÄ Makefile                  # Build configuration
‚îÇ       ‚îú‚îÄ‚îÄ main.cpp       
‚îÇ       ‚îú‚îÄ‚îÄ Query.cpp
        ‚îú‚îÄ‚îÄ dapgalg.h

‚îÇ                         
‚îÇ       
‚îú‚îÄ‚îÄ dataset/                         
‚îú‚îÄ‚îÄ .gitattributes                   
‚îú‚îÄ‚îÄ .gitignore                       
‚îú‚îÄ‚îÄ LICENSE                          
‚îî‚îÄ‚îÄ README.md                       
```
## Contact

For questions or contributions, please open an issue or contact the authors listed in the paper.


